{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from models/ggml-model-q4_0.bin\n",
      "llama.cpp: can't use mmap because tensors are not aligned; convert to new format to avoid this\n",
      "llama_model_load_internal: format     = ggmf v1 (old version with no mmap support)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 4113739.11 KB\n",
      "llama_model_load_internal: mem required  = 5809.32 MB (+ 2052.00 MB per state)\n",
      "....................................................................................................\n",
      "llama_init_from_file: kv self size  =  512.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n",
      "llama.cpp: loading model from models/ggml-model-q4_0.bin\n",
      "llama.cpp: can't use mmap because tensors are not aligned; convert to new format to avoid this\n",
      "llama_model_load_internal: format     = ggmf v1 (old version with no mmap support)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 4113739.11 KB\n",
      "llama_model_load_internal: mem required  = 5809.32 MB (+ 1026.00 MB per state)\n",
      "....................................................................................................\n",
      "llama_init_from_file: kv self size  =  256.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n",
      "Exception ignored in: <function Llama.__del__ at 0x7f2f56db0310>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/langchain/lib/python3.10/site-packages/llama_cpp/llama.py\", line 905, in __del__\n",
      "    if self.ctx is not None:\n",
      "AttributeError: 'Llama' object has no attribute 'ctx'\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "TextSplitter.__init__() got an unexpected keyword argument 'separator'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 49\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[39mreturn\u001b[39;00m matched_docs, sources\n\u001b[1;32m     48\u001b[0m docs \u001b[39m=\u001b[39m loader\u001b[39m.\u001b[39mload()\n\u001b[0;32m---> 49\u001b[0m chunks \u001b[39m=\u001b[39m split_chunks(docs)\n\u001b[1;32m     50\u001b[0m embeddings \u001b[39m=\u001b[39m generate_embedding(chunks)\n\u001b[1;32m     52\u001b[0m question \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mWhat are the use cases of LangChain?\u001b[39m\u001b[39m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[6], line 17\u001b[0m, in \u001b[0;36msplit_chunks\u001b[0;34m(sources)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msplit_chunks\u001b[39m(sources: \u001b[39mlist\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mlist\u001b[39m:\n\u001b[1;32m     16\u001b[0m     chunks \u001b[39m=\u001b[39m []\n\u001b[0;32m---> 17\u001b[0m     splitter \u001b[39m=\u001b[39m RecursiveCharacterTextSplitter(separator\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m, chunk_size\u001b[39m=\u001b[39;49m\u001b[39m256\u001b[39;49m, chunk_overlap\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m)\n\u001b[1;32m     18\u001b[0m     \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m splitter\u001b[39m.\u001b[39msplit_documents(sources):\n\u001b[1;32m     19\u001b[0m         chunks\u001b[39m.\u001b[39mappend(chunk)\n",
      "File \u001b[0;32m/opt/conda/envs/langchain/lib/python3.10/site-packages/langchain/text_splitter.py:265\u001b[0m, in \u001b[0;36mRecursiveCharacterTextSplitter.__init__\u001b[0;34m(self, separators, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, separators: Optional[List[\u001b[39mstr\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any):\n\u001b[1;32m    264\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Create a new TextSplitter.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    266\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_separators \u001b[39m=\u001b[39m separators \u001b[39mor\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: TextSplitter.__init__() got an unexpected keyword argument 'separator'"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.document_loaders import UnstructuredHTMLLoader\n",
    "from langchain.embeddings import LlamaCppEmbeddings\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain.document_loaders import PDFMinerLoader, TextLoader\n",
    "\n",
    "# loader = UnstructuredHTMLLoader(\"langchain/docs/_build/html/index.html\")\n",
    "loader = PDFMinerLoader(\"./arxiv_papers/gpt4-papers/2203.02155.pdf\")\n",
    "embedding = LlamaCppEmbeddings(model_path=\"models/ggml-model-q4_0.bin\")\n",
    "llm = LlamaCpp(model_path=\"models/ggml-model-q4_0.bin\")\n",
    "\n",
    "\n",
    "def split_chunks(sources: list) -> list:\n",
    "    chunks = []\n",
    "    splitter = RecursiveCharacterTextSplitter(separator=\"\", chunk_size=256, chunk_overlap=16)\n",
    "    for chunk in splitter.split_documents(sources):\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def generate_embedding(chunks: list):\n",
    "    texts = [doc.page_content for doc in chunks]\n",
    "    metadatas = [doc.metadata for doc in chunks]\n",
    "\n",
    "    search_index = FAISS.from_texts(texts, embedding, metadatas=metadatas)\n",
    "\n",
    "    return search_index\n",
    "\n",
    "\n",
    "def similarity_search(\n",
    "        query: str, index: FAISS\n",
    "):\n",
    "    matched_docs = index.similarity_search(query, k=4)\n",
    "    sources = []\n",
    "    for doc in matched_docs:\n",
    "        sources.append(\n",
    "            {\n",
    "                \"page_content\": doc.page_content,\n",
    "                \"metadata\": doc.metadata,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return matched_docs, sources\n",
    "\n",
    "\n",
    "docs = loader.load()\n",
    "chunks = split_chunks(docs)\n",
    "embeddings = generate_embedding(chunks)\n",
    "\n",
    "question = \"What are the use cases of LangChain?\"\n",
    "matched_docs, sources = similarity_search(question, embeddings)\n",
    "\n",
    "template = \"\"\"\n",
    "Please use the following context to answer questions.\n",
    "Context: {context}\n",
    "---\n",
    "Question: {question}\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "context = \"\\n\".join([doc.page_content for doc in matched_docs])\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"]).partial(context=context)\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "1. Techniques to improve prompt reliability, OpenAI [https://github.com/openai/openai-cookbook/blob/main/techniques_to_improve_reliability.md#how-to-improve-reliability-on-complex-tasks]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
