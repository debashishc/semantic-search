{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Retrieval Demo\n",
    "\n",
    "Date: 26 April 2023\n",
    "\n",
    "In this demo, we will be demo-ing how to perform Information Retrieval using Langchain.\n",
    "\n",
    "### What is LangChain\n",
    "\n",
    "LangChain is a framework for developing applications powered by language models. \n",
    "\n",
    "_LangChain Principles:_\n",
    "- Be data-aware: connect a language model to other sources of data\n",
    "- Be agentic: allow a language model to interact with its environment\n",
    "\n",
    "Source: [LangChain documentation](https://python.langchain.com/en/latest/index.html)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Retrieval\n",
    "\n",
    "<img src=\"https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4063347e-8920-40c6-86b3-c520084b303c_1272x998.jpeg\" alt= “” width=\"600\" height=\"500\">\n",
    "\n",
    "\n",
    "<!-- ![](https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4063347e-8920-40c6-86b3-c520084b303c_1272x998.jpeg) -->\n",
    "\n",
    "Source: [Finetuning Large Language Models by Sebastian Raschka](https://magazine.sebastianraschka.com/p/finetuning-large-language-models)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain: Models\n",
    "\n",
    "Generic inferface for models e.g. LLMs, Chat Models, Text Embedding Models. Read more [here](https://python.langchain.com/en/latest/modules/models.html)\n",
    "\n",
    "```python\n",
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI(model_name=\"text-ada-001\")\n",
    "\n",
    "from langchain import HuggingFaceHub\n",
    "llm = HuggHuggingFaceHub(repo_id=\"google/flan-t5-xl\")\n",
    "\n",
    "from langchain.llms import Cohere\n",
    "llm = Cohere()\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Models: HuggingFace Pipeline\n",
    "\n",
    "Note: This is experimental code, always use functional or OO abstraction for your implementation.\n",
    "\n",
    "```python\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, pipeline, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_id = 'flan-t5-large' # any local model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id, load_in_8bit=True) # 8bit in A10, A100 etc.\n",
    "\n",
    "pipe = pipeline(\n",
    "    task=\"text2text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_length=100\n",
    ")\n",
    "\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain: Prompts\n",
    "\n",
    "Prompt Management, Optimization and Serialization. Read more [here](https://docs.langchain.com/docs/components/prompts)\n",
    "\n",
    "```python\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Let's think step by step.\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "user_input = input(\"Enter your question: \")\n",
    "prompt.format(question=user_input)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain: Chain\n",
    "\n",
    "Sequence of calls (with multiple models). Read more [here](https://python.langchain.com/en/latest/modules/chains.html)  \n",
    "[] `LLMChain`  \n",
    "[] `SequentialChain`  \n",
    "[] Custom `Chain`  \n",
    "\n",
    "```python\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0.9)\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"Which country is the largest producer of {product}?\",\n",
    ")\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "# Chain a LLM with a prompt template\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Run the chain only specifying the input variable.\n",
    "print(chain.run(\"lithium\"))\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```python\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "q = \"What type of mammal lays the biggest eggs?\"\n",
    "\n",
    "llm = OpenAI(temperature=0.7)\n",
    "template = \"\"\"{question}\\n\\n\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"question\"], template=template)\n",
    "question_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "template = \"\"\"Here is a statement:\n",
    "{statement}\n",
    "Make a bullet point list of the assumptions you made when producing the above statement.\\n\\n\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"statement\"], template=template)\n",
    "assumptions_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "template = \"\"\"Here is a bullet point list of assertions:\n",
    "{assertions}\n",
    "For each assertion, determine whether it is true or false. If it is false, explain why.\\n\\n\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"assertions\"], template=template)\n",
    "fact_checker_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "template = \"\"\"In light of the above facts, how would you answer the question '{}'\"\"\".format(q)\n",
    "template = \"\"\"{facts}\\n\"\"\" + template\n",
    "prompt_template = PromptTemplate(input_variables=[\"facts\"], template=template)\n",
    "answer_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "overall_chain = SimpleSequentialChain(chains=[question_chain, assumptions_chain, fact_checker_chain, answer_chain], verbose=True)\n",
    "```\n",
    "\n",
    "Credit to jagilley/fact-checker for the example. Check the repo [here](https://github.com/jagilley/fact-checker)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![Importance of SeqChains](https://weaviate.io/assets/images/sequential-chains-fec82f27b64a0d8f5b6123b39569ecf2.gif)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain: Agents\n",
    "\n",
    "Agents that can use tools like Google Search or Wikipedia. Read more [here](https://python.langchain.com/en/latest/modules/agents.html)\n",
    "\n",
    "Note: This is _not_ great for air gapped systems.\n",
    "\n",
    "```python\n",
    "from langchain.agents import load_tools, initialize_agent, AgentType\n",
    "from langchain.agents import \n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "tools = load_tools([\"google-search\", \"wikimedia\", \"llm-math\"], \n",
    "                    llm=llm)\n",
    "agent = initialize_agent(tools=tools,\n",
    "                        llm=llm,\n",
    "                        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "                        verbose=True)\n",
    "\n",
    "agent.run(\"Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?\") #relevant example\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain: Memory\n",
    "\n",
    "Momory components for\n",
    "1. utilities for managing and manipulating previous chat messages\n",
    "2. incorporate these utilities into chains\n",
    "\n",
    "Read more [here](https://python.langchain.com/en/latest/modules/memory.html)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain: Indexes\n",
    "\n",
    "Utility functions to combine own private data. Read more [here](https://python.langchain.com/en/latest/modules/indexes.html).\n",
    "\n",
    "Now we are talking.\n",
    "\n",
    "### [Document Loaders](https://python.langchain.com/en/latest/modules/indexes/document_loaders.html)\n",
    "\n",
    "Email, Images, PDF, s3 Directory and Files, Word Documents, Powerpoints etc.\n",
    "\n",
    "### [Text Splitters](https://python.langchain.com/en/latest/modules/indexes/text_splitters.html)\n",
    "\n",
    "Character Text Splitter, Recursive Character Text Splitter here.\n",
    "\n",
    "### [VectorStores](https://python.langchain.com/en/latest/modules/indexes/vectorstores.html)\n",
    "\n",
    "ElasticSearch, FAISS, Qdrant, Redis, Weaviate\n",
    "\n",
    "### [Retrievers](https://python.langchain.com/en/latest/modules/indexes/retrievers.html)\n",
    "\n",
    "Contextual Compression Retriever (!!!), SVM Retriever, TF-IDF Retriever, Time Weighted VectorStore Retriever etc.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain: Indexes\n",
    "\n",
    "```python\n",
    "query = <YOUR_QUERY_HERE>\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "db_faiss = FAISS.from_documents(documents=docs, \n",
    "                                embedding=embeddings)\n",
    "docs = db_faiss.similarity_search(query)\n",
    "\n",
    "from langchain.vectorstores import ElasticVectorSearch\n",
    "db_elastic = ElasticVectorSearch.from_documents(documents=docs, \n",
    "                                                embedding=embeddings, \n",
    "                                                elasticsearch_url=\"http://localhost:9200\")\n",
    "docs = db_elastic.similarity_search(query)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_id = 'google/flan-t5-large'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id,)# load_in_8bit=True)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_length=100\n",
    ")\n",
    "\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paris\n"
     ]
    }
   ],
   "source": [
    "print(local_llm('What is the capital of France? '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "switzerland\n"
     ]
    }
   ],
   "source": [
    "print(local_llm(\"Which country is the biggest producer of wine?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The United States is the biggest producer of wine. The United States is the largest producer of wine in the world. So, the answer is United States.\n"
     ]
    }
   ],
   "source": [
    "print(local_llm(\"Which country is the biggest producer of wine? \\n Let's think step by step.\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The biggest producer of wine is France. France makes a total of 1.2 billion bottles of wine a year. The second biggest producer of wine is Italy. Italy makes 1.2 billion bottles of wine a year. The answer: Italy.\n"
     ]
    }
   ],
   "source": [
    "print(local_llm(\"Which country is the biggest producer of wine? How much wine does this country make compared to second biggest producer of wine?\\n Let's think step by step.\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] LangChain Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
